{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.14)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: absl-py in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jax->mediapipe) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jax->mediapipe) (1.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vikas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Determining Joints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/3j8BPdc.png\" style=\"height:300px\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Calculate Angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pose-Based Fitness Tracker project to calculate the angle between body joints, such as the elbow, shoulder, and wrist, during bicep curls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Curl Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Pose Landmarks Detection Works in MediaPipe\n",
    "\n",
    "MediaPipe's **Pose Landmarks Detection** system is a real-time pose estimation technique that predicts the position of key body joints (or landmarks) in images or video frames. It uses a combination of machine learning and deep learning techniques. Here’s a breakdown of how it works:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Input Image Processing\n",
    "- The input to the pose detection system is an image (or a sequence of frames in a video) that is preprocessed.\n",
    "- The image is resized and normalized to fit the model's input size requirements.\n",
    "\n",
    "### 2. Pose Detection Model\n",
    "- MediaPipe utilizes a deep learning model, typically a **Convolutional Neural Network (CNN)**, that has been trained on a large dataset of human poses.\n",
    "- The model first performs **region detection**, identifying whether a person (or parts of the body) is visible in the image.\n",
    "- It creates a **bounding box** around the detected person.\n",
    "\n",
    "### 3. Landmark Prediction\n",
    "- Once a person is detected, the system predicts **33 body landmarks**, which include key points like the:\n",
    "  - Shoulders\n",
    "  - Elbows\n",
    "  - Wrists\n",
    "  - Hips\n",
    "  - Knees\n",
    "  - Ankles\n",
    "  \n",
    "  Each landmark point is represented by 3 coordinates:\n",
    "  - **x**: Horizontal position in the image.\n",
    "  - **y**: Vertical position in the image.\n",
    "  - **z**: Depth (indicating how far the point is from the camera).\n",
    "\n",
    "### 4. Model Architecture\n",
    "- MediaPipe’s Pose model employs a **two-step process**:\n",
    "  1. **Detector**: First, a lightweight detector identifies the region of interest (person) in the image.\n",
    "  2. **Pose Estimator**: Then, a more refined network predicts the precise locations of landmarks within the detected region.\n",
    "\n",
    "### 5. Coordinate Transformation\n",
    "- The model outputs normalized coordinates, where **x** and **y** are relative to the image dimensions (ranging from 0 to 1). The **z** coordinate is relative to the depth of the body parts, which helps to understand 3D movements.\n",
    "\n",
    "### 6. Real-Time Processing\n",
    "- One of MediaPipe’s strengths is its **efficiency**. The system is optimized to perform landmark detection in real-time, even on mobile devices, by balancing accuracy and computational cost.\n",
    "\n",
    "### 7. Tracking Across Frames\n",
    "- When dealing with videos, MediaPipe continuously tracks the detected landmarks across frames, allowing for **smooth pose estimation** and **real-time feedback**. This is useful for applications like fitness tracking, dance movement analysis, or even augmented reality (AR).\n",
    "\n",
    "---\n",
    "\n",
    "### Applications of Pose Landmarks Detection\n",
    "- **Fitness tracking**: To track body movements and count exercise repetitions.\n",
    "- **Gesture recognition**: To recognize gestures or specific movements.\n",
    "- **Animation and gaming**: To capture human motion for character animation.\n",
    "- **Health and rehabilitation**: To analyze posture and body mechanics.\n",
    "\n",
    "---\n",
    "\n",
    "This detection process allows for capturing detailed, real-time body movement, enabling a wide range of applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Curl counter variables\n",
    "counter = 0 \n",
    "stage = None\n",
    "\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Get coordinates\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            \n",
    "            # Calculate angle\n",
    "            angle = calculate_angle(shoulder, elbow, wrist)\n",
    "            \n",
    "            # Visualize angle\n",
    "            cv2.putText(image, str(angle), \n",
    "                           tuple(np.multiply(elbow, [640, 480]).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "            \n",
    "            # Curl counter logic\n",
    "            if angle > 160:\n",
    "                stage = \"down\"\n",
    "            if angle < 30 and stage =='down':\n",
    "                stage=\"up\"\n",
    "                counter +=1\n",
    "                print(counter)\n",
    "                       \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Render curl counter\n",
    "        # Setup status box\n",
    "        cv2.rectangle(image, (0,0), (225,73), (245,117,16), -1)\n",
    "        \n",
    "        # Rep data\n",
    "        cv2.putText(image, 'REPS', (15,12), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(counter), \n",
    "                    (10,60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Stage data\n",
    "        cv2.putText(image, 'STAGE', (65,12), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, stage, \n",
    "                    (60,60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose Landmarks Detection (Human Pose)\n",
    "\n",
    "If you're using pose detection (like in your case), MediaPipe uses a trained neural network to detect key **landmarks** on the human body. These landmarks represent joints or key points like the shoulder, elbow, wrist, hip, etc. It predicts the positions of these points relative to the image dimensions.\n",
    "\n",
    "#### Pose Landmarks Model\n",
    "This model is based on deep learning and is optimized for efficiency, allowing real-time processing. It outputs the **x, y, and z coordinates** for multiple body points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Required Libraries\n",
    "In this section, we import essential libraries:\n",
    "\n",
    "- `cv2`: OpenCV for video capture and image processing.\n",
    "- `numpy`: For mathematical operations.\n",
    "- `mediapipe`: For pose estimation.\n",
    "\n",
    "### 2. Helper Function for Angle Calculation\n",
    "This function calculates the angle between three points (shoulder, elbow, wrist). \n",
    "The angle is crucial for detecting bicep curls in the real-time video.\n",
    "\n",
    "### 3. Pose Estimation and Curl Counter Setup\n",
    "This block initializes the video capture from the webcam and sets up the MediaPipe pose estimation model.\n",
    "It also sets up variables for counting bicep curls (`counter`) and tracking the movement stage (`stage`).\n",
    "\n",
    "### 4. Main Loop for Curl Detection\n",
    "The main loop does the following:\n",
    "\n",
    "- Continuously captures frames from the webcam.\n",
    "- Converts frames to RGB for pose detection.\n",
    "- Uses MediaPipe to detect body landmarks (shoulder, elbow, wrist).\n",
    "- Calculates the angle between these landmarks.\n",
    "- Implements logic to count curls when the arm goes from a \"down\" to \"up\" position.\n",
    "- Displays the number of reps and the current stage (\"up\" or \"down\") on the video feed.\n",
    "\n",
    "### 5. Closing Video Capture\n",
    "After pressing the 'q' key, the video capture ends, and the window closes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: MediaPipe vs. OpenPose\n",
    "\n",
    "**MediaPipe** and **OpenPose** are both popular frameworks for pose estimation and related tasks. Below is a comparison highlighting why MediaPipe is often considered more efficient:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Real-Time Performance**\n",
    "\n",
    "- **MediaPipe**:\n",
    "  - **Optimized for Real-Time**: MediaPipe is designed specifically for real-time performance, making it suitable for applications like live video processing.\n",
    "  - **Low Latency**: It achieves low-latency processing through optimized models and pipelines, which is crucial for interactive applications.\n",
    "\n",
    "- **OpenPose**:\n",
    "  - **Higher Latency**: OpenPose, while highly accurate, often has higher latency due to its more complex models and processing requirements.\n",
    "  - **Performance**: It requires significant computational resources, which may affect real-time performance, especially on lower-end devices.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Resource Efficiency**\n",
    "\n",
    "- **MediaPipe**:\n",
    "  - **Lightweight Models**: MediaPipe uses lightweight models that are optimized to run efficiently on a wide range of devices, including mobile and web.\n",
    "  - **Adaptive Resource Usage**: It adapts to the hardware capabilities, ensuring efficient use of available resources.\n",
    "\n",
    "- **OpenPose**:\n",
    "  - **Resource-Intensive**: OpenPose models are generally more resource-intensive, requiring powerful GPUs and high computational power.\n",
    "  - **Hardware Dependency**: It performs best on high-end GPUs, which may not be available in all devices.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Cross-Platform Support**\n",
    "\n",
    "- **MediaPipe**:\n",
    "  - **Versatile Deployment**: Supports multiple platforms, including mobile (iOS, Android), web (via WebAssembly), and desktop.\n",
    "  - **Unified Framework**: Provides a consistent framework across different environments, making it easier to deploy and integrate.\n",
    "\n",
    "- **OpenPose**:\n",
    "  - **Limited Cross-Platform Support**: Primarily designed for desktop environments, with more complex setup and less support for mobile or web platforms.\n",
    "  - **Platform Constraints**: Deployment on different platforms can be challenging due to its heavy dependencies and computational requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Ease of Use and Integration**\n",
    "\n",
    "- **MediaPipe**:\n",
    "  - **High-Level APIs**: Offers high-level APIs and tools that simplify the development process.\n",
    "  - **Pre-Built Solutions**: Comes with pre-trained models for various tasks, reducing the need for extensive custom model training.\n",
    "\n",
    "- **OpenPose**:\n",
    "  - **Complex Setup**: Requires more complex setup and configuration, including dependencies and model training.\n",
    "  - **Development Effort**: Integrating OpenPose into applications can be more challenging due to its complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Customizability**\n",
    "\n",
    "- **MediaPipe**:\n",
    "  - **Modular Design**: Allows for the creation of custom pipelines and easy integration with other systems.\n",
    "  - **Flexible Pipelines**: Users can customize and extend existing models to fit specific use cases.\n",
    "\n",
    "- **OpenPose**:\n",
    "  - **Less Modular**: While powerful, OpenPose's architecture is less modular, and customizing or extending the model can be more involved.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Community and Support**\n",
    "\n",
    "- **MediaPipe**:\n",
    "  - **Active Development**: Maintained and developed by Google with active updates and improvements.\n",
    "  - **Community Contributions**: Benefits from contributions and feedback from a large community.\n",
    "\n",
    "- **OpenPose**:\n",
    "  - **Active but Limited**: Maintained by the Carnegie Mellon Perceptual Computing Lab with a dedicated community but less frequent updates compared to MediaPipe.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "**MediaPipe** is often considered more efficient due to its real-time performance, resource efficiency, cross-platform support, ease of use, and modularity. While **OpenPose** offers high accuracy and comprehensive features, its higher computational requirements and complexity can make it less suitable for real-time applications and less resource-constrained environments.\n",
    "\n",
    "---\n",
    "\n",
    "Both frameworks have their strengths and are suitable for different applications. The choice between them depends on specific needs, such as the importance of real-time performance versus accuracy, and the available hardware resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Challenges and Solutions\n",
    "\n",
    "### Challenges Addressed\n",
    "\n",
    "1. **Real-Time Pose Detection**\n",
    "   - **Problem:** Efficiently detecting and tracking key landmarks on the human body in real-time using video feed.\n",
    "   - **Solution:** Utilized MediaPipe's Pose model, which is optimized for real-time processing. This model provides accurate detection of key points such as shoulders, elbows, and wrists, which are crucial for tracking exercises.\n",
    "\n",
    "2. **Accurate Angle Calculation**\n",
    "   - **Problem:** Determining the angle between body joints (e.g., shoulder, elbow, and wrist) to count repetitions of exercises like bicep curls.\n",
    "   - **Solution:** Implemented a `calculate_angle` function to compute the angle formed by three points. This function uses trigonometric calculations to measure the angle and adjust it if it exceeds 180 degrees.\n",
    "\n",
    "3. **Rep Counting and Stage Detection**\n",
    "   - **Problem:** Counting the number of exercise repetitions and detecting different stages of the exercise (e.g., upward and downward phases of a curl).\n",
    "   - **Solution:** Applied angle thresholds to distinguish between different stages of the exercise. The counter is incremented when the exercise transitions from the downward to the upward phase.\n",
    "\n",
    "4. **Handling Video Capture and Display**\n",
    "   - **Problem:** Managing video capture from a webcam and displaying results in a user-friendly manner.\n",
    "   - **Solution:** Used OpenCV to capture video frames and display the real-time feed with overlaid text and visualizations. Ensured proper handling of video frames and GUI rendering.\n",
    "\n",
    "5. **User Interface for Interaction**\n",
    "   - **Problem:** Providing a user-friendly interface for starting and stopping the workout.\n",
    "   - **Solution:** Developed a simple GUI using Tkinter, allowing users to start and stop the workout session. Included buttons to control the workout and display feedback messages.\n",
    "\n",
    "6. **Thread Management and Graceful Termination**\n",
    "   - **Problem:** Ensuring the workout session can be stopped gracefully and without leaving resources open.\n",
    "   - **Solution:** Implemented threading with a stop flag to manage the workout session. The flag signals the workout thread to terminate, and the thread is joined to ensure clean exit.\n",
    "\n",
    "7. **Data Recording and Feedback**\n",
    "   - **Problem:** Saving workout data and providing feedback to the user.\n",
    "   - **Solution:** Created a log file to record workout details, including the number of curls and timestamps. Added message boxes to inform users when the workout starts, stops, and completes.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Your project effectively addressed the challenges of real-time pose detection, angle calculation, rep counting, and user interaction. By leveraging MediaPipe for pose detection and OpenCV for video handling, along with a Tkinter-based GUI for user control, you created a robust fitness tracker that provides real-time feedback and ensures a seamless user experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of the Fitness Tracking Project\n",
    "\n",
    "### 1. **Personal Fitness Training**\n",
    "   - **Description:** Provides individuals with real-time feedback on their exercise form and repetition count, helping them to perform exercises correctly and effectively. Ideal for users looking to improve their workout routines at home.\n",
    "\n",
    "### 2. **Physical Rehabilitation**\n",
    "   - **Description:** Assists patients undergoing physical rehabilitation by monitoring their exercise progress and ensuring proper technique. Useful for physiotherapists to track patient adherence to prescribed exercises.\n",
    "\n",
    "### 3. **Sports Coaching**\n",
    "   - **Description:** Enables coaches to analyze athletes' performance by providing detailed feedback on exercise form and counting repetitions. Helps in optimizing training regimens and preventing injuries.\n",
    "\n",
    "### 4. **Fitness App Integration**\n",
    "   - **Description:** Can be integrated into mobile or desktop fitness applications to offer users an interactive way to track their workouts and monitor their progress. Enhances user engagement by providing real-time exercise feedback.\n",
    "\n",
    "### 5. **Virtual Personal Training**\n",
    "   - **Description:** Supports virtual personal training sessions by allowing trainers to monitor and guide clients remotely. Facilitates personalized training plans and real-time corrections.\n",
    "\n",
    "### 6. **Gym Management Systems**\n",
    "   - **Description:** Can be implemented in gym management systems to offer automated feedback and performance tracking for gym-goers. Provides valuable data for gym operators to offer enhanced services and support.\n",
    "\n",
    "### 7. **Research and Development**\n",
    "   - **Description:** Useful for researchers studying exercise biomechanics, human motion analysis, and machine learning applications in fitness. Provides a foundation for developing advanced exercise tracking technologies.\n",
    "\n",
    "### 8. **Health Monitoring Devices**\n",
    "   - **Description:** Can be incorporated into wearable health monitoring devices to offer exercise tracking and performance analysis as part of a comprehensive health monitoring solution.\n",
    "\n",
    "### 9. **Educational Tools**\n",
    "   - **Description:** Serves as an educational tool for teaching students about computer vision, pose estimation, and fitness technology. Provides hands-on experience with real-world applications of these technologies.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The fitness tracking project has diverse applications across personal fitness, rehabilitation, coaching, and technology integration. Its ability to provide real-time feedback and track exercise performance makes it a valuable tool for improving fitness outcomes and supporting various health and fitness-related initiatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
